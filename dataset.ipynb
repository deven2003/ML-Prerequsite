{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmVgRZ57dUgkVeuwsCJdZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deven2003/ML-Prerequsite/blob/main/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-iCP-SN5hR2",
        "outputId": "b3092263-e4a9-4c49-80dd-93711c17035a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Combining the training data\n",
            "Done Combining the testing data\n",
            "Done Combining the data\n"
          ]
        }
      ],
      "source": [
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "#\n",
        "#                                   ES335- Machine Learning- Assignment 1\n",
        "#\n",
        "# This script combines the data from the UCI HAR Dataset into a more usable format.\n",
        "# The data is combined into a single csv file for each subject and activity.\n",
        "# The data is then stored in the Combined folder.\n",
        "#\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "# Library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Give the path of the test and train folder of UCI HAR Dataset\n",
        "train_path = \"UCI HAR Dataset/train\"\n",
        "test_path = \"UCI HAR Dataset/test\"\n",
        "\n",
        "# Dictionary of activities. Provided by the dataset.\n",
        "ACTIVITIES = {\n",
        "    1: 'WALKING'            ,\n",
        "    2: 'WALKING_UPSTAIRS'   ,\n",
        "    3: 'WALKING_DOWNSTAIRS' ,\n",
        "    4: 'SITTING'            ,\n",
        "    5: 'STANDING'           ,\n",
        "    6: 'LAYING'             ,\n",
        "}\n",
        "\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "                                        # Combining Traing Data\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "# Load all the accelerometer data\n",
        "total_acc_x = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_x_train.txt\"),delim_whitespace=True,header=None)\n",
        "total_acc_y = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_y_train.txt\"),delim_whitespace=True,header=None)\n",
        "total_acc_z = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_z_train.txt\"),delim_whitespace=True,header=None)\n",
        "\n",
        "\n",
        "# Read the subject IDs\n",
        "subject_train = pd.read_csv(os.path.join(train_path,\"subject_train.txt\"),delim_whitespace=True,header=None)\n",
        "\n",
        "# Read the labels\n",
        "y = pd.read_csv(os.path.join(train_path,\"y_train.txt\"),delim_whitespace=True,header=None)\n",
        "\n",
        "\n",
        "# Toggle through all the subjects.\n",
        "for subject in np.unique(subject_train.values):\n",
        "\n",
        "    sub_idxs = np.where( subject_train.iloc[:,0] == subject )[0]\n",
        "    labels = y.loc[sub_idxs]\n",
        "\n",
        "    # Toggle through all the labels.\n",
        "    for label in np.unique(labels.values):\n",
        "\n",
        "        # make the folder directory if it does not exist\n",
        "        if not os.path.exists(os.path.join(\"Combined\",\"Train\",ACTIVITIES[label])):\n",
        "            os.makedirs(os.path.join(\"Combined\",\"Train\",ACTIVITIES[label]))\n",
        "\n",
        "        label_idxs = labels[labels.iloc[:,0] == label].index\n",
        "\n",
        "        accx = []\n",
        "        accy = []\n",
        "        accz = []\n",
        "\n",
        "        for idx in label_idxs:\n",
        "            if accx is not None:\n",
        "                accx = np.hstack((accx,total_acc_x.loc[idx][64:]))\n",
        "                accy = np.hstack((accy,total_acc_y.loc[idx][64:]))\n",
        "                accz = np.hstack((accz,total_acc_z.loc[idx][64:]))\n",
        "\n",
        "            else:\n",
        "                accx = total_acc_x.loc[idx]\n",
        "                accy = total_acc_y.loc[idx]\n",
        "                accz = total_acc_z.loc[idx]\n",
        "\n",
        "        # saving the data into csv file\n",
        "        data = pd.DataFrame({'accx':accx,'accy':accy,'accz':accz})\n",
        "        save_path = os.path.join(\"Combined\",\"Train\",ACTIVITIES[label],f\"Subject_{subject}.csv\")\n",
        "        data.to_csv(save_path,index=False)\n",
        "\n",
        "print(\"Done Combining the training data\")\n",
        "\n",
        "\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "                                        # Combining Test Data\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "# Load all the accelerometer data\n",
        "total_acc_x = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_x_test.txt\"),delim_whitespace=True,header=None)\n",
        "total_acc_y = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_y_test.txt\"),delim_whitespace=True,header=None)\n",
        "total_acc_z = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_z_test.txt\"),delim_whitespace=True,header=None)\n",
        "\n",
        "# Read the subject IDs\n",
        "subject_test = pd.read_csv(os.path.join(test_path,\"subject_test.txt\"),delim_whitespace=True,header=None)\n",
        "\n",
        "# Read the labels\n",
        "y = pd.read_csv(os.path.join(test_path,\"y_test.txt\"),delim_whitespace=True,header=None)\n",
        "\n",
        "# Toggle through all the subjects.\n",
        "for subject in np.unique(subject_test.values):\n",
        "\n",
        "        sub_idxs = np.where( subject_test.iloc[:,0] == subject )[0]\n",
        "        labels = y.loc[sub_idxs]\n",
        "\n",
        "        # Toggle through all the labels.\n",
        "        for label in np.unique(labels.values):\n",
        "\n",
        "            if not os.path.exists(os.path.join(\"Combined\",\"Test\",ACTIVITIES[label])):\n",
        "                os.makedirs(os.path.join(\"Combined\",\"Test\",ACTIVITIES[label]))\n",
        "\n",
        "            label_idxs = labels[labels.iloc[:,0] == label].index\n",
        "\n",
        "            accx = []\n",
        "            accy = []\n",
        "            accz = []\n",
        "            for idx in label_idxs:\n",
        "                if accx is not None:\n",
        "                    accx = np.hstack((accx,total_acc_x.loc[idx][64:]))\n",
        "                    accy = np.hstack((accy,total_acc_y.loc[idx][64:]))\n",
        "                    accz = np.hstack((accz,total_acc_z.loc[idx][64:]))\n",
        "\n",
        "                else:\n",
        "                    accx = total_acc_x.loc[idx]\n",
        "                    accy = total_acc_y.loc[idx]\n",
        "                    accz = total_acc_z.loc[idx]\n",
        "\n",
        "            # saving the data into csv file\n",
        "            data = pd.DataFrame({'accx':accx,'accy':accy,'accz':accz})\n",
        "            save_path = os.path.join(\"Combined\",\"Test\",ACTIVITIES[label],f\"Subject_{subject}.csv\")\n",
        "            data.to_csv(save_path,index=False)\n",
        "\n",
        "print(\"Done Combining the testing data\")\n",
        "print(\"Done Combining the data\")\n",
        "\n",
        "#-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "#\n",
        "#                                   ES335- Machine Learning- Assignment 1\n",
        "#\n",
        "# This file is used to create the dataset for the mini-project. The dataset is created by reading the data from\n",
        "# the Combined folder. The data is then split into training, testing, and validation sets. This split is supposed\n",
        "# to be used for all the modeling purposes.\n",
        "#\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "# Library imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Constants\n",
        "time = 10\n",
        "offset = 100\n",
        "folders = [\"LAYING\",\"SITTING\",\"STANDING\",\"WALKING\",\"WALKING_DOWNSTAIRS\",\"WALKING_UPSTAIRS\"]\n",
        "classes = {\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6}\n",
        "\n",
        "combined_dir = os.path.join(\"Combined\")\n",
        "\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "                                                # Train Dataset\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "dataset_dir = os.path.join(combined_dir,\"Train\")\n",
        "\n",
        "for folder in folders:\n",
        "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
        "        df = df[offset:offset+time*50]\n",
        "        X_train.append(df.values)\n",
        "        y_train.append(classes[folder])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "                                                # Test Dataset\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "X_test=[]\n",
        "y_test=[]\n",
        "dataset_dir = os.path.join(combined_dir,\"Test\")\n",
        "\n",
        "for folder in folders:\n",
        "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
        "    for file in files:\n",
        "\n",
        "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
        "        df = df[offset:offset+time*50]\n",
        "        X_test.append(df.values)\n",
        "        y_test.append(classes[folder])\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "                                                # Final Dataset\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "# USE THE BELOW GIVEN DATA FOR TRAINING, TESTING, AND VALIDATION purposes\n",
        "\n",
        "# concatenate the training and testing data\n",
        "X = np.concatenate((X_train,X_test))\n",
        "y = np.concatenate((y_train,y_test))\n",
        "\n",
        "# split the data into training,testing, and validation sets\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=4,stratify=y)\n",
        "X_test,X_val,y_test,y_val = train_test_split(X_test,y_test,test_size=0.5,random_state=4,stratify=y_test)\n",
        "\n",
        "print(\"Training data shape: \",X_train.shape)\n",
        "print(\"Testing data shape: \",X_test.shape)\n",
        "print(\"Validation data shape: \",X_val.shape)\n",
        "\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ8_ugly8Jii",
        "outputId": "594d0e72-7704-4dc6-cbce-3c6bf382e33c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape:  (108, 500, 3)\n",
            "Testing data shape:  (36, 500, 3)\n",
            "Validation data shape:  (36, 500, 3)\n"
          ]
        }
      ]
    }
  ]
}